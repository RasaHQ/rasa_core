
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Training and Policies</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/banner.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Debugging" href="../debugging/" />
    <link rel="prev" title="Fallback Actions" href="../fallbacks/" />

 <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-MMHSZCS');</script>
    <!-- End Google Tag Manager -->
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
  <meta itemprop="image" content="https://rasa.com/assets/img/facebook-og.png">
  <meta property="og:title" content="Training and Policies" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://rasa.com/assets/img/facebook-og.png" />
  <meta property="og:url" content="https://rasa.com/docs/core/policies" />
  
    <meta name="description" content="Understanding Rasa Core Policies" />
    <meta itemprop="description" content="Understanding Rasa Core Policies">
    <meta name="twitter:description" content="Understanding Rasa Core Policies" />
    <meta property="og:description" content="Understanding Rasa Core Policies" />
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@Rasa_HQ">
  <meta name="twitter:title" content="Training and Policies">
  <meta name="twitter:creator" content="@Rasa_HQ">
  <meta name="twitter:image" content="https://rasa.com/assets/img/facebook-og.png">

  <link rel="stylesheet" href="../_static/xq-light.css" type="text/css" />
  <link rel="stylesheet" href="../_static/fontawesome/css/fontawesome-all.css" type="text/css" />
  <!--<script defer type="text/javascript" src="../_static/config.js"></script>
  <script defer type="text/javascript" src="../_static/klaro.js"></script>-->
  <script type="text/javascript" src="https://storage.googleapis.com/docs-theme/clipboard.min.js"></script>
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />


  </head><body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MMHSZCS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<div class="nav-top">
  <div class="nav-container">
    <a href="/docs" class="brand-link">
        <h1 class="brand">
            <img src="../_static/rasa_logo.svg" width="80px" height="40px" title="Rasa logo" alt="Rasa logo">
        </h1>
        <span class="logo extension">docs</span>
    </a>

    <ul class="nav">
      <input type="text" class="search" placeholder="Search">
      
        
          <li><a href=/docs/nlu/>NLU</a></li>
        
      
        
          <li class="active"><a href=/docs/core/>Core</a></li>
        
      
        
          <li><a href=/docs/platform/>Platform</a></li>
        
      

      <li>
        <a href="https://forum.rasa.com" target="_blank"><button class="button btn-ghost btn-small">Ask the Community</button></a>
      </li>

    </ul>
  </div>
</div>

  <div class="sidebar-extended"></div>
  <div class="document">

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../architecture/">High-Level Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../connectors/">Chat &amp; Voice platforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../customactions/">Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slots/">Using Slots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slotfilling/">Slot Filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../responses/">Bot Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interactive_learning/">Interactive Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fallbacks/">Fallback Actions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training and Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging/">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation/">Evaluating and Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker_walkthrough/">Development with Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../domains/">Domain Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stories/">Story Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/slots_api/">Slot Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server/">HTTP API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/agent/">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/events/">Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/tracker/">Dialogue State Tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/interpreter/">Interpreters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/policy/">Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/featurizer/">Featurization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/dispatcher/">Dispatcher</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migrations/">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tracker_stores/">Tracker Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brokers/">Event Brokers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker/">Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog/">Change Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support/">Getting Support</a></li>
</ul>

<div class="versions">
    <p class="caption">Versions</p>
    <div class="versions-content">
      <div>
        <span class="current-version">
          viewing: 0.12.1
        </span>
      </div>
      <div class="other-versions">
          <p>tags</p>
          <div class="dropdown-content">
              <a href="../../0.12.1/policies/">0.12.1</a>
              <a href="../../0.12.0/policies/">0.12.0</a>
              <a href="../../0.11.12/policies/">0.11.12</a>
              <a href="../../0.10.4/policies/">0.10.4</a>
              <a href="../../0.9.8/policies/">0.9.8</a>
              <a href="../../0.8.6/policies/">0.8.6</a>
              <a href="../../0.7.9/policies/">0.7.9</a>
              <a href="../../0.6.9/policies/">0.6.9</a>
          </div>
          <p>branches</p>
          <div class="dropdown-content">
              <a href="../../master/policies/">master</a>
          </div>
      </div>
    </div>
</div>


        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  





<div class="section" id="training-and-policies">
<span id="policies"></span><h1>Training and Policies<a class="headerlink" href="#training-and-policies" title="Permalink to this headline">¶</a></h1>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>Rasa Core works by creating training data from your stories and
training a model on that data.</p>
<p>You can run training from the command line like in the <a class="reference internal" href="../quickstart/#quickstart"><span class="std std-ref">Quickstart</span></a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m rasa_core.train -d domain.yml -s data/stories.md <span class="se">\</span>
  -o models/current/dialogue -c config.yml
</pre></div>
</div>
<p>Or by creating an agent and running the train method yourself:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rasa_core.agent</span> <span class="kn">import</span> <span class="n">Agent</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;stories.md&quot;</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="default-configuration">
<span id="default-config"></span><h2>Default configuration<a class="headerlink" href="#default-configuration" title="Permalink to this headline">¶</a></h2>
<p>By default, we try to provide you with a good set of configuration values
and policies that suit most people. But you are encouraged to modify
these to your needs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">policies</span><span class="p">:</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">KerasPolicy</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="mi">100</span>
    <span class="n">max_history</span><span class="p">:</span> <span class="mi">5</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">FallbackPolicy</span>
    <span class="n">fallback_action_name</span><span class="p">:</span> <span class="s1">&#39;action_default_fallback&#39;</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">MemoizationPolicy</span>
    <span class="n">max_history</span><span class="p">:</span> <span class="mi">5</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">FormPolicy</span>
</pre></div>
</div>
<div class="section" id="data-augmentation">
<h3>Data Augmentation<a class="headerlink" href="#data-augmentation" title="Permalink to this headline">¶</a></h3>
<p>By default, Rasa Core will create longer stories by randomly glueing together
the ones in your stories file. This is because if you have stories like:</p>
<div class="highlight-story notranslate"><div class="highlight"><pre><span></span><span class="gh"># thanks</span>
<span class="vm">* thankyou</span>
<span class="vm">   </span>- utter_youarewelcome

<span class="gh"># bye</span>
<span class="vm">* goodbye</span>
<span class="vm">   </span>- utter_goodbye
</pre></div>
</div>
<p>You actually want to teach your policy to <strong>ignore</strong> the dialogue history
when it isn’t relevant and just respond with the same action no matter
what happened before.</p>
<p>You can alter this behaviour with the <code class="docutils literal notranslate"><span class="pre">--augmentation</span></code> flag.
<code class="docutils literal notranslate"><span class="pre">--augmentation</span> <span class="pre">0</span></code> disables this behavior.</p>
<p>In python, you can pass the <code class="docutils literal notranslate"><span class="pre">augmentation_factor</span></code> argument to the
<code class="docutils literal notranslate"><span class="pre">Agent.load_data</span></code> method.</p>
</div>
<div class="section" id="max-history">
<h3>Max History<a class="headerlink" href="#max-history" title="Permalink to this headline">¶</a></h3>
<p>One important hyperparameter for Rasa Core policies is the <code class="docutils literal notranslate"><span class="pre">max_history</span></code>.
This controls how much dialogue history the model looks at to decide which
action to take next.</p>
<p>You can set the <code class="docutils literal notranslate"><span class="pre">max_history</span></code> by passing it to your policy’s <code class="docutils literal notranslate"><span class="pre">Featurizer</span></code>
in the policy configuration yaml file.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Only the <code class="docutils literal notranslate"><span class="pre">MaxHistoryTrackerFeaturizer</span></code> uses a max history,
whereas the <code class="docutils literal notranslate"><span class="pre">FullDialogueTrackerFeaturizer</span></code> always looks at
the full conversation history.</p>
</div>
<p>As an example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">out_of_scope</span></code> intent which
describes off-topic user messages. If your bot sees this intent multiple
times in a row, you might want to tell the user what you <cite>can</cite> help them
with. So your story might look like this:</p>
<div class="highlight-story notranslate"><div class="highlight"><pre><span></span><span class="vm">* out_of_scope</span>
<span class="vm">   </span>- utter_default
<span class="vm">* out_of_scope</span>
<span class="vm">   </span>- utter_default
<span class="vm">* out_of_scope</span>
<span class="vm">   </span>- utter_help_message
</pre></div>
</div>
<p>For Rasa Core to learn this pattern, the <code class="docutils literal notranslate"><span class="pre">max_history</span></code>
has to be <cite>at least</cite> <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p>
<p>If you increase your <code class="docutils literal notranslate"><span class="pre">max_history</span></code>, your model will become bigger and
training will take longer. If you have some information that should
affect the dialogue very far into the future, you should store it as a
slot. Slot information is always available for every featurizer.</p>
</div>
<div class="section" id="training-script-options">
<h3>Training Script Options<a class="headerlink" href="#training-script-options" title="Permalink to this headline">¶</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>/home/travis/virtualenv/python3.5.6/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/home/travis/virtualenv/python3.5.6/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/home/travis/virtualenv/python3.5.6/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/home/travis/virtualenv/python3.5.6/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
usage: train.py default [-h] [--augmentation AUGMENTATION] [--dump_stories]
                        [--debug_plots] [-v] [-vv] [--quiet] [-c CONFIG] -o
                        OUT (-s STORIES | --url URL) -d DOMAIN

optional arguments:
  -h, --help            show this help message and exit
  --augmentation AUGMENTATION
                        how much data augmentation to use during training
  --dump_stories        If enabled, save flattened stories to a file
  --debug_plots         If enabled, will create plots showing checkpoints and
                        their connections between story blocks in a file
                        called `story_blocks_connections.pdf`.
  -v, --verbose         Be verbose. Sets logging level to INFO
  -vv, --debug          Print lots of debugging statements. Sets logging level
                        to DEBUG
  --quiet               Be quiet! Sets logging level to WARNING
  -c CONFIG, --config CONFIG
                        Policy specification yaml file.
  -o OUT, --out OUT     directory to persist the trained model in
  -s STORIES, --stories STORIES
                        File or folder containing stories
  --url URL             If supplied, downloads a story file from a URL and
                        trains on it. Fetches the data by sending a GET
                        request to the supplied URL.
  -d DOMAIN, --domain DOMAIN
                        Domain specification (yml file)
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2>Policies<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="../api/policy/#rasa_core.policies.Policy" title="rasa_core.policies.Policy"><code class="xref py py-class docutils literal notranslate"><span class="pre">rasa_core.policies.Policy</span></code></a> class decides which action to take
at every step in the conversation.</p>
<p>There are different policies to choose from, and you can include
multiple policies in a single <a class="reference internal" href="../api/agent/#rasa_core.agent.Agent" title="rasa_core.agent.Agent"><code class="xref py py-class docutils literal notranslate"><span class="pre">rasa_core.agent.Agent</span></code></a>. At
every turn, the policy which predicts the next action with the
highest confidence will be used.</p>
<div class="section" id="configuring-polices-using-a-configuration-file">
<span id="policy-file"></span><h3>Configuring polices using a configuration file<a class="headerlink" href="#configuring-polices-using-a-configuration-file" title="Permalink to this headline">¶</a></h3>
<p>If you are using the training script, you must set the policies you would like
the Core model to use in a YAML file.</p>
<p>For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">policies</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="s">&quot;KerasPolicy&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">featurizer</span><span class="p p-Indicator">:</span>
    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">MaxHistoryTrackerFeaturizer</span>
      <span class="l l-Scalar l-Scalar-Plain">max_history</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
      <span class="l l-Scalar l-Scalar-Plain">state_featurizer</span><span class="p p-Indicator">:</span>
        <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">BinarySingleStateFeaturizer</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="s">&quot;MemoizationPolicy&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">max_history</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="s">&quot;FallbackPolicy&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">nlu_threshold</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">0.4</span>
    <span class="l l-Scalar l-Scalar-Plain">core_threshold</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">0.3</span>
    <span class="l l-Scalar l-Scalar-Plain">fallback_action_name</span><span class="p p-Indicator">:</span> <span class="s">&quot;my_fallback_action&quot;</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="s">&quot;path.to.your.policy.class&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">arg1</span><span class="p p-Indicator">:</span> <span class="s">&quot;...&quot;</span>
</pre></div>
</div>
<p>Pass the YAML file’s name to the train script using the <code class="docutils literal notranslate"><span class="pre">--config</span></code>
argument (or just <code class="docutils literal notranslate"><span class="pre">-c</span></code>). There is a default config file you can use to
get started: <a class="reference internal" href="#default-config"><span class="std std-ref">Default configuration</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Policies specified higher in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> will take
precedence over a policy specified lower if the confidences
are equal.</p>
</div>
</div>
<div class="section" id="configuring-polices-in-code">
<h3>Configuring polices in code<a class="headerlink" href="#configuring-polices-in-code" title="Permalink to this headline">¶</a></h3>
<p>You can pass a list of policies when you create an agent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rasa_core.policies.memoization</span> <span class="kn">import</span> <span class="n">MemoizationPolicy</span>
<span class="kn">from</span> <span class="nn">rasa_core.policies.keras_policy</span> <span class="kn">import</span> <span class="n">KerasPolicy</span>
<span class="kn">from</span> <span class="nn">rasa_core.agent</span> <span class="kn">import</span> <span class="n">Agent</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="s2">&quot;domain.yml&quot;</span><span class="p">,</span>
               <span class="n">policies</span><span class="o">=</span><span class="p">[</span><span class="n">MemoizationPolicy</span><span class="p">(),</span> <span class="n">KerasPolicy</span><span class="p">()])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default, Rasa Core uses the <code class="docutils literal notranslate"><span class="pre">KerasPolicy</span></code> in combination with
the <code class="docutils literal notranslate"><span class="pre">MemoizationPolicy</span></code>.</p>
</div>
</div>
<div class="section" id="memoization-policy">
<h3>Memoization Policy<a class="headerlink" href="#memoization-policy" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">MemoizationPolicy</span></code> just memorizes the conversations in your
training data. It predicts the next action with confidence <code class="docutils literal notranslate"><span class="pre">1.0</span></code>
if this exact conversation exists in the training data, otherwise it
predicts <code class="docutils literal notranslate"><span class="pre">None</span></code> with confidence <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p>
</div>
<div class="section" id="keras-policy">
<h3>Keras Policy<a class="headerlink" href="#keras-policy" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">KerasPolicy</span></code> uses a neural network implemented in
<a class="reference external" href="http://keras.io">Keras</a> to select the next action.
The default architecture is based on an LSTM, but you can override the
<code class="docutils literal notranslate"><span class="pre">KerasPolicy.model_architecture</span></code> method to implement your own architecture.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">model_architecture</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="p">,</span>  <span class="c1"># type: Tuple[int, int]</span>
            <span class="n">output_shape</span>  <span class="c1"># type: Tuple[int, Optional[int]]</span>
    <span class="p">):</span>
        <span class="c1"># type: (...) -&gt; tf.keras.models.Sequential</span>
        <span class="sd">&quot;&quot;&quot;Build a keras model and return a compiled model.&quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
        <span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> \
            <span class="n">Masking</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">Activation</span>

        <span class="c1"># Build Model</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

        <span class="c1"># the shape of the y vector of the labels,</span>
        <span class="c1"># determines which output from rnn will be used</span>
        <span class="c1"># to calculate the loss</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># y is (num examples, num features) so</span>
            <span class="c1"># only the last output from the rnn is used to</span>
            <span class="c1"># calculate the loss</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Masking</span><span class="p">(</span><span class="n">mask_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">output_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># y is (num examples, max_dialogue_len, num features) so</span>
            <span class="c1"># all the outputs from the rnn are used to</span>
            <span class="c1"># calculate the loss, therefore a sequence is returned and</span>
            <span class="c1"># time distributed layer is used</span>

            <span class="c1"># the first value in input_shape is max dialogue_len,</span>
            <span class="c1"># it is set to None, to allow dynamic_rnn creation</span>
            <span class="c1"># during prediction</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Masking</span><span class="p">(</span><span class="n">mask_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">output_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot construct the model because&quot;</span>
                             <span class="s2">&quot;length of output_shape = </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;should be 1 or 2.&quot;</span>
                             <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)))</span>

        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                      <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
                      <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>and the training is run here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">training_trackers</span><span class="p">,</span>  <span class="c1"># type: List[DialogueStateTracker]</span>
              <span class="n">domain</span><span class="p">,</span>  <span class="c1"># type: Domain</span>
              <span class="o">**</span><span class="n">kwargs</span>  <span class="c1"># type: Any</span>
              <span class="p">):</span>
        <span class="c1"># type: (...) -&gt; Dict[Text: Any]</span>

        <span class="n">training_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">featurize_for_training</span><span class="p">(</span><span class="n">training_trackers</span><span class="p">,</span>
                                                    <span class="n">domain</span><span class="p">,</span>
                                                    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># noinspection PyPep8Naming</span>
        <span class="n">shuffled_X</span><span class="p">,</span> <span class="n">shuffled_y</span> <span class="o">=</span> <span class="n">training_data</span><span class="o">.</span><span class="n">shuffled_X_y</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_architecture</span><span class="p">(</span><span class="n">shuffled_X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                                                         <span class="n">shuffled_y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Fitting model with </span><span class="si">{}</span><span class="s2"> total samples and a &quot;</span>
                            <span class="s2">&quot;validation split of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="n">training_data</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">validation_split</span><span class="p">))</span>
                <span class="c1"># filter out kwargs that cannot be passed to fit</span>
                <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_valid_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">shuffled_X</span><span class="p">,</span> <span class="n">shuffled_y</span><span class="p">,</span>
                               <span class="n">epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                               <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                               <span class="o">**</span><span class="n">params</span><span class="p">)</span>
                <span class="c1"># the default parameter for epochs in keras fit is 1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Done fitting keras policy model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can implement the model of your choice by overriding these methods,
or initialize <code class="docutils literal notranslate"><span class="pre">KerasPolicy</span></code> with pre-defined <code class="docutils literal notranslate"><span class="pre">keras</span> <span class="pre">model</span></code>.</p>
</div>
</div>
<div class="section" id="embedding-policy">
<span id="id2"></span><h2>Embedding policy<a class="headerlink" href="#embedding-policy" title="Permalink to this headline">¶</a></h2>
<p>This policy has a pre-defined architecture, which comprises the
following steps:</p>
<blockquote>
<div><ul class="simple">
<li>apply dense layers to create embeddings for user intents,
entities and system actions including previous actions and slots;</li>
<li>use the embeddings of previous user inputs as a user memory
and embeddings of previous system actions as a system memory;</li>
<li>concatenate user input, previous system action and slots
embeddings for current time into an input vector to rnn;</li>
<li>using user and previous system action embeddings from the input
vector, calculate attention probabilities over the user and
system memories (for system memory, this policy uses
<a class="reference external" href="https://arxiv.org/abs/1410.5401">NTM mechanism</a> with attention
by location);</li>
<li>sum the user embedding and user attention vector and feed it
and the embeddings of the slots as an input to an LSTM cell;</li>
<li>apply a dense layer to the output of the LSTM to get a raw
recurrent embedding of a dialogue;</li>
<li>sum this raw recurrent embedding of a dialogue with system
attention vector to create dialogue level embedding, this step
allows the algorithm to repeat previous system action by copying
its embedding vector directly to the current time output;</li>
<li>weight previous LSTM states with system attention probabilities
to get the previous action embedding, the policy is likely payed
attention to;</li>
<li>if the similarity between this previous action embedding and
current time dialogue embedding is high, overwrite current LSTM
state with the one from the time when this action happened;</li>
<li>for each LSTM time step, calculate the similarity between the
dialogue embedding and embedded system actions.
This step is based on the starspace idea from:
<a class="reference external" href="https://arxiv.org/abs/1709.03856">https://arxiv.org/abs/1709.03856</a>.</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This policy only works with
<code class="docutils literal notranslate"><span class="pre">FullDialogueTrackerFeaturizer(state_featurizer)</span></code>.</p>
</div>
<p>It is recommended to use
<code class="docutils literal notranslate"><span class="pre">state_featurizer=LabelTokenizerSingleStateFeaturizer(...)</span></code>
(see <a class="reference internal" href="../api/featurizer/#featurization"><span class="std std-ref">Featurization</span></a> for details).</p>
<p><strong>Configuration</strong>:</p>
<blockquote>
<div><blockquote>
<div><p>Configuration parameters can be passed to <code class="docutils literal notranslate"><span class="pre">agent.train(...)</span></code> method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Pass an appropriate <code class="docutils literal notranslate"><span class="pre">epochs</span></code> number to <code class="docutils literal notranslate"><span class="pre">agent.train(...)</span></code>
method, otherwise the policy will be trained only for <code class="docutils literal notranslate"><span class="pre">1</span></code> epoch.
Since this is embedding based policy, it requires a large
number of epochs, which depends on the complexity of the
training data and whether attention is used or not.</p>
</div>
<p>The main feature of this policy is an <strong>attention</strong> mechanism over
previous user input and system actions.
<strong>Attention is turned on by default</strong>, in order to turn it off,
configure the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">attn_before_rnn</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> the algorithm will use
attention mechanism over previous user input, default <code class="docutils literal notranslate"><span class="pre">true</span></code>;</li>
<li><code class="docutils literal notranslate"><span class="pre">attn_after_rnn</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> the algorithm will use
attention mechanism over previous system actions and will be
able to copy previously executed action together with LSTM’s
hidden state from its history, default <code class="docutils literal notranslate"><span class="pre">true</span></code>;</li>
<li><code class="docutils literal notranslate"><span class="pre">sparse_attention</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> <code class="docutils literal notranslate"><span class="pre">sparsemax</span></code> will be used
instead of <code class="docutils literal notranslate"><span class="pre">softmax</span></code> for attention probabilities, default
<code class="docutils literal notranslate"><span class="pre">false</span></code>;</li>
<li><code class="docutils literal notranslate"><span class="pre">attn_shift_range</span></code> the range of allowed location-based
attention shifts for system memory (<code class="docutils literal notranslate"><span class="pre">attn_after_rnn</span></code>), see
<a class="reference external" href="https://arxiv.org/abs/1410.5401">https://arxiv.org/abs/1410.5401</a> for details;</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Attention requires larger values of <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and takes longer
to train. But it can learn more complicated and nonlinear behaviour.</p>
</div>
<p>The algorithm also has hyper-parameters to control:</p>
<blockquote>
<div><ul>
<li><p class="first">neural network’s architecture:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes_a</span></code> sets a list of hidden layers
sizes before embedding layer for user inputs, the number
of hidden layers is equal to the length of the list;</li>
<li><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes_b</span></code> sets a list of hidden layers
sizes before embedding layer for system actions, the number
of hidden layers is equal to the length of the list;</li>
<li><code class="docutils literal notranslate"><span class="pre">rnn_size</span></code> sets the number of units in the LSTM cell;</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">training:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">layer_norm</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> layer normalization for lstm
cell is turned on,  default <code class="docutils literal notranslate"><span class="pre">true</span></code>;</li>
<li><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> sets the number of training examples in one
forward/backward pass, the higher the batch size, the more
memory space you’ll need;</li>
<li><code class="docutils literal notranslate"><span class="pre">epochs</span></code> sets the number of times the algorithm will see
training data, where <code class="docutils literal notranslate"><span class="pre">one</span> <span class="pre">epoch</span></code> = one forward pass and
one backward pass of all the training examples;</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">embedding:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> sets the dimension of embedding space;</li>
<li><code class="docutils literal notranslate"><span class="pre">mu_pos</span></code> controls how similar the algorithm should try
to make embedding vectors for correct intent labels;</li>
<li><code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> controls maximum negative similarity for
incorrect intents;</li>
<li><code class="docutils literal notranslate"><span class="pre">similarity_type</span></code> sets the type of the similarity,
it should be either <code class="docutils literal notranslate"><span class="pre">cosine</span></code> or <code class="docutils literal notranslate"><span class="pre">inner</span></code>;</li>
<li><code class="docutils literal notranslate"><span class="pre">num_neg</span></code> sets the number of incorrect intent labels,
the algorithm will minimize their similarity to the user
input during training;</li>
<li><code class="docutils literal notranslate"><span class="pre">use_max_sim_neg</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> the algorithm only
minimizes maximum similarity over incorrect intent labels;</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">regularization:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">C2</span></code> sets the scale of L2 regularization</li>
<li><code class="docutils literal notranslate"><span class="pre">C_emb</span></code> sets the scale of how important is to minimize
the maximum similarity between embeddings of different
intent labels;</li>
<li><code class="docutils literal notranslate"><span class="pre">droprate_a</span></code> sets the dropout rate between hidden
layers before embedding layer for user inputs;</li>
<li><code class="docutils literal notranslate"><span class="pre">droprate_b</span></code> sets the dropout rate between hidden layers
before embedding layer for system actions;</li>
<li><code class="docutils literal notranslate"><span class="pre">droprate_rnn</span></code> sets the recurrent dropout rate on
the LSTM hidden state <a class="reference external" href="https://arxiv.org/abs/1603.05118">https://arxiv.org/abs/1603.05118</a>;</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">train accuracy calculation:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">evaluate_every_num_epochs</span></code> sets how often to calculate
train accuracy, small values may hurt performance;</li>
<li><code class="docutils literal notranslate"><span class="pre">evaluate_on_num_examples</span></code> how many examples to use for
calculation of train accuracy, large values may hurt
performance.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Droprate should be between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, e.g.
<code class="docutils literal notranslate"><span class="pre">droprate=0.1</span></code> would drop out <code class="docutils literal notranslate"><span class="pre">10%</span></code> of input units.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For <code class="docutils literal notranslate"><span class="pre">cosine</span></code> similarity <code class="docutils literal notranslate"><span class="pre">mu_pos</span></code> and <code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> should
be between <code class="docutils literal notranslate"><span class="pre">-1</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is an option to use linearly increasing batch size.
The idea comes from <a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a>.
In order to do it pass a list to <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g.
<code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">[8,</span> <span class="pre">32]</span></code> (default behaviour). If constant
<code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is required, pass an <code class="docutils literal notranslate"><span class="pre">int</span></code>, e.g.
<code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">8</span></code>.</p>
</div>
<p>These parameters can be passed to <code class="docutils literal notranslate"><span class="pre">Agent.train(...)</span></code> method.
The default values are defined in <code class="docutils literal notranslate"><span class="pre">EmbeddingPolicy.defaults</span></code>:</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">defaults</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># nn architecture</span>
        <span class="c1"># a list of hidden layers sizes before user embed layer</span>
        <span class="c1"># number of hidden layers is equal to the length of this list</span>
        <span class="s2">&quot;hidden_layers_sizes_a&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="c1"># a list of hidden layers sizes before bot embed layer</span>
        <span class="c1"># number of hidden layers is equal to the length of this list</span>
        <span class="s2">&quot;hidden_layers_sizes_b&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="c1"># number of units in rnn cell</span>
        <span class="s2">&quot;rnn_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>

        <span class="c1"># training parameters</span>
        <span class="c1"># flag if to turn on layer normalization for lstm cell</span>
        <span class="s2">&quot;layer_norm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># initial and final batch sizes - batch size will be</span>
        <span class="c1"># linearly increased for each epoch</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
        <span class="c1"># number of epochs</span>
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>

        <span class="c1"># embedding parameters</span>
        <span class="c1"># dimension size of embedding vectors</span>
        <span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="c1"># how similar the algorithm should try</span>
        <span class="c1"># to make embedding vectors for correct actions</span>
        <span class="s2">&quot;mu_pos&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
        <span class="c1"># maximum negative similarity for incorrect actions</span>
        <span class="s2">&quot;mu_neg&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
        <span class="c1"># the type of the similarity</span>
        <span class="s2">&quot;similarity_type&quot;</span><span class="p">:</span> <span class="s1">&#39;cosine&#39;</span><span class="p">,</span>  <span class="c1"># string &#39;cosine&#39; or &#39;inner&#39;</span>
        <span class="c1"># the number of incorrect actions, the algorithm will minimize</span>
        <span class="c1"># their similarity to the user input during training</span>
        <span class="s2">&quot;num_neg&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="c1"># flag if minimize only maximum similarity over incorrect actions</span>
        <span class="s2">&quot;use_max_sim_neg&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># flag which loss function to use</span>

        <span class="c1"># regularization</span>
        <span class="c1"># the scale of L2 regularization</span>
        <span class="s2">&quot;C2&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="c1"># the scale of how important is to minimize the maximum similarity</span>
        <span class="c1"># between embeddings of different actions</span>
        <span class="s2">&quot;C_emb&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
        <span class="c1"># scale loss with inverse frequency of bot actions</span>
        <span class="s2">&quot;scale_loss_by_action_counts&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>

        <span class="c1"># dropout rate for user nn</span>
        <span class="s2">&quot;droprate_a&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="c1"># dropout rate for bot nn</span>
        <span class="s2">&quot;droprate_b&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="c1"># dropout rate for rnn</span>
        <span class="s2">&quot;droprate_rnn&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>

        <span class="c1"># attention parameters</span>
        <span class="c1"># flag to use attention over user input</span>
        <span class="c1"># as an input to rnn</span>
        <span class="s2">&quot;attn_before_rnn&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># flag to use attention over prev bot actions</span>
        <span class="c1"># and copy it to output bypassing rnn</span>
        <span class="s2">&quot;attn_after_rnn&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>

        <span class="c1"># flag to use `sparsemax` instead of `softmax` for attention</span>
        <span class="s2">&quot;sparse_attention&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># flag to use sparsemax for probs</span>
        <span class="c1"># the range of allowed location-based attention shifts</span>
        <span class="s2">&quot;attn_shift_range&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># if None, set to mean dialogue length / 2</span>

        <span class="c1"># visualization of accuracy</span>
        <span class="c1"># how often calculate train accuracy</span>
        <span class="s2">&quot;evaluate_every_num_epochs&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>  <span class="c1"># small values may hurt performance</span>
        <span class="c1"># how many examples to use for calculation of train accuracy</span>
        <span class="s2">&quot;evaluate_on_num_examples&quot;</span><span class="p">:</span> <span class="mi">100</span>  <span class="c1"># large values may hurt performance</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Parameter <code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> is set to a negative value to mimic
the original starspace algorithm in the case
<code class="docutils literal notranslate"><span class="pre">mu_neg</span> <span class="pre">=</span> <span class="pre">mu_pos</span></code> and <code class="docutils literal notranslate"><span class="pre">use_max_sim_neg</span> <span class="pre">=</span> <span class="pre">False</span></code>. See
<a class="reference external" href="https://arxiv.org/abs/1709.03856">starspace paper</a> for details.</p>
</div>
</div></blockquote>
</div>
<div class="section" id="have-questions-or-feedback">
<h2>Have questions or feedback?<a class="headerlink" href="#have-questions-or-feedback" title="Permalink to this headline">¶</a></h2>
<p>We have a very active support community on <a class="reference external" href="https://forum.rasa.com">Rasa Community Forum</a>
that is happy to help you with your questions. If you have any feedback for us or a specific
suggestion for improving the docs, feel free to share it by creating an issue on Rasa Core
<a class="reference external" href="https://github.com/RasaHQ/rasa_core/issues">GitHub repository</a>.</p>
</div>
</div>


	    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
	    <script type="text/javascript"> docsearch({
	     apiKey: '1f9e0efb89e98543f6613a60f847b176',
	     indexName: 'rasa',
	     inputSelector: 'body > div.nav-top > .nav-container > .nav > input',
	     debug: false // Set debug to true if you want to inspect the dropdown
	    });
	    </script>
          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2018, Rasa Technologies GmbH | <a href="https://rasa.com/imprint/" target="_blank">Imprint</a> | <a href="https://rasa.com/privacy-policy/" target="_blank">Privacy Policy</a>
      

    </div>

    

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag(...arguments) {
      dataLayer.push(...arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-87333416-1', {
      'anonymize_ip': true,
    });
  </script>
  <script src="https://rasa.com/assets/js/js.cookies.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- TODO: Re-Enable once Klaro is re-activated again. Remove the next two lines, then.
  <script type="opt-in" data-name="googleanalytics" data-type="text/javascript" data-src="https://www.googletagmanager.com/gtag/js?id=UA-87333416-1"></script>
  <script type="opt-in" data-name="googleanalytics" data-type="text/javascript" data-src="https://rasa.com/assets/js/userId.js"></script>
  -->
  <script src="https://www.googletagmanager.com/gtag/js?id=UA-87333416-1"></script>
  <script src="https://rasa.com/assets/js/userId.js"></script>
  <script type="text/javascript">
    var clipboard = new ClipboardJS('.copyable');
    clipboard.on('success', function(e) {
      gtag('event', e.action, {
        'event_category': 'code',
        'event_label': e.text
      });
      const id = e.text.slice(0,3);
      document.getElementById(id).classList.add('visible');
      setTimeout(function(){
        document.getElementById(id).classList.remove('visible');},
        800
      );
    });
    clipboard.on('error', function(e) {
      console.log(e);
    });
  </script>

  <!-- onsite anchor fix (otherwise anchors scroll to far) -->
  <script>
    /* Adapted from https://stackoverflow.com/a/13067009/1906073 */
    (function(document, history, location) {
      var HISTORY_SUPPORT = !!(history && history.pushState);

      var anchorScrolls = {
        ANCHOR_REGEX: /^#[^ ]+$/,
        OFFSET_HEIGHT_PX: 66,

        /**
         * Establish events, and fix initial scroll position if a hash is provided.
         */
        init: function() {
          this.scrollToCurrent();
          $(window).on('hashchange', $.proxy(this, 'scrollToCurrent'));
          $('body').on('click', 'a', $.proxy(this, 'delegateAnchors'));
        },

        /**
         * Return the offset amount to deduct from the normal scroll position.
         * Modify as appropriate to allow for dynamic calculations
         */
        getFixedOffset: function() {
          return this.OFFSET_HEIGHT_PX;
        },

        /**
         * If the provided href is an anchor which resolves to an element on the
         * page, scroll to it.
         * @param  {String} href
         * @return {Boolean} - Was the href an anchor.
         */
        scrollIfAnchor: function(href, pushToHistory) {
          var match, anchorOffset;

          if(!this.ANCHOR_REGEX.test(href)) {
            return false;
          }

          match = document.getElementById(href.slice(1));

          if(match) {
            anchorOffset = $(match).offset().top - this.getFixedOffset();
            $('html, body').animate({ scrollTop: anchorOffset});

            // Add the state to history as-per normal anchor links
            if(HISTORY_SUPPORT && pushToHistory) {
              history.pushState({}, document.title, location.pathname + href);
            }
          }

          return !!match;
        },

        /**
         * Attempt to scroll to the current location's hash.
         */
        scrollToCurrent: function(e) {
          if(this.scrollIfAnchor(window.location.hash) && e) {
            e.preventDefault();
          }
        },

        /**
         * If the click event's target was an anchor, fix the scroll position.
         */
        delegateAnchors: function(e) {
          var elem = e.target;

          if(this.scrollIfAnchor(elem.getAttribute('href'), true)) {
            e.preventDefault();
          }
        }
      };

      $(document).ready($.proxy(anchorScrolls, 'init'));
    })(window.document, window.history, window.location);

  </script>
  <div id="webchat">
  <script src="https://storage.googleapis.com/mrbot-cdn/webchat-0.4.2.js"></script>
  <script>
   WebChat.default.init({
       selector: "#webchat",
       initPayload: "/greet",
       interval: 150,
       socketUrl: "https://website-demo.rasa.com/",
       title: "Sara",
       subtitle: "I'm still in development",
       profileAvatar: "https://rasa.com/assets/img/demo/sara_avatar.png",
       showCloseButton: true,
       fullScreenMode: false,})
  </script>
  </div>
  </body>
</html>